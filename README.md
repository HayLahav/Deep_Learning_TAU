# Deep_Learning_TAU course

# HW1:  Lenet5 network over the FashionMNIST data set
The primary goal is to evaluate the impact of three specific techniques when applied to Lenet5:
Dropout: This involves adding dropout layers to the hidden layers of the Lenet5 network to reduce overfitting.
Weight Decay (L2 Loss): Weight decay, also known as L2 regularization, is employed to control overfitting by adding a regularization term to the network's loss function.
Batch Normalization: Batch normalization is introduced to normalize the activations within each layer, which can accelerate training and improve the network's generalization.

